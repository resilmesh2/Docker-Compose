from huggingface_hub import InferenceClient
from typing import List, Dict, Any, Tuple, Optional, TypedDict, Union
from enum import Enum
from pydantic import Field
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate
from dataclasses import dataclass, field
import json
import re
from .pentest_types import Task


class PentestState(TypedDict):
    iteration: int
    pending_tasks: List[Task]  # Temporary working list (reused for different phases)
    completed_tasks: List[Task]  # Tasks that finished execution but need memory processing
    failed_tasks: Optional[List]
    current_phase: str  # "execution" | "memory_saving" | "reproting"
    errors: List[str]
    graph_data: List[Dict] 


@dataclass
class Task:
    task_id: int
    description: str
    target: str
    task_type: str
    dependencies: List[int] = field(default_factory=list)
    command: Optional[str] = None
    command_output: Optional[str] = None
    error: Optional[str] = None
    error_count: Optional[int] = 0
    task_summary: Optional[str] = None

@dataclass(slots=True)
class PentestNode:
    """Lightweight node representation with type hints"""
    ip: str
    info: List[Dict[str, Any]] = field(default_factory=list)
    task_history: List[Dict[str, Any]] = field(default_factory=list)

    def set_info(self, task_id: str, task_type: str, instruction: str, memory: str) -> None:
        """Adds a new info entry as a dictionary with id, instruction, and memory"""
        self.info.append({
            "id": task_id, 
            "task_type": task_type,
            "instruction": instruction,
            "memory": memory
        })

    def get_info(self) -> str:
        """Returns a formatted string of all info entries"""
        if not self.info:
            return "No info available."

        result = []
        for entry in self.info:
            result.append(
                f"Task {entry['id']} â€” you asked the EXECUTOR \"{entry['instruction']}\". "
                f"The MEMORY AGENT found: {entry['memory']}."
            )
        return "\n".join(result)
    
    def get_node(self) -> list:
        return self.info
    
    def add_task(self, task: Dict[str, Any]) -> None:
        """Simplified task history tracking"""
        self.task_history.append(task)

    def get_last_task(self) -> Dict[str, Any]:
        return self.task_history[-1] if self.task_history else None

class LLMtoCommand:
    def parse_json(self, text: str) -> tuple:
        """
        Returns tuple: (parsed_data, error_message)
        - On success: (dict or list, None)
        - On failure: (None, error_string)
        """
        json_patterns = [
            r'```(?:json)?\s*(\{.*\}|\[.*\])\s*```',  # Triple backticks (with or without "json")
            r'`(\{.*\}|\[.*\])`',                    # Single backticks
            r'(\{.*\}|\[.*\])'                       # Standalone JSON
        ]
        
        for pattern in json_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                json_str = match.group(1).strip()
                try:
                    return json.loads(json_str), None
                except json.JSONDecodeError as e:
                    return None, f"JSON parsing error: {str(e)}"
                except Exception as e:
                    return None, f"Unexpected error: {str(e)}"

        return None, "No JSON data found in text"

    def parse_nmap(self, text: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Extracts and validates an nmap command from the given text.
        Returns tuple: (command, error_message)
        - On success: (command_string, None)
        - On failure: (None, error_string)
        """
        nmap_patterns = [
            r'```(?:shell)?\s*(nmap\s+.*?)\s*```',  # Triple backticks (with or without "shell")
            r'`(nmap\s+.*?)`',                      # Single backticks
            r'(nmap\s+.*?)\s*$'                     # Standalone nmap command
        ]
        
        for pattern in nmap_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                command = match.group(1).strip()
                try:
                    # Basic validation
                    if not command.startswith('nmap'):
                        return None, "Only nmap commands are allowed"
                    
                    # Check for dangerous characters
                    if any(char in command for char in [';', '&', '|', '$', '>', '<']):
                        return None, "Dangerous characters detected"
                    
                    return command, None
                except Exception as e:
                    return None, f"Parsing error: {str(e)}"

        return None, "No nmap command found in text"
    
    def parse_msf(self, text: str) -> Tuple[Optional[str], Optional[str]]:
            """
            Extracts and validates an msfconsole command from the given text.
            Returns tuple: (command, error_message)
            - On success: (command_string, None)
            - On failure: (None, error_string)
            """
            msf_patterns = [
                r'```(?:shell)?\s*(msfconsole\s+-q\s+-x\s+["\'].*?["\'])\s*```',  # Triple backticks (shell block)
                r'`(msfconsole\s+-q\s+-x\s+["\'].*?["\'])`',                      # Single backticks
                r'(msfconsole\s+-q\s+-x\s+["\'].*?["\'])\s*$'                     # Standalone msfconsole command
            ]
            
            for pattern in msf_patterns:
                match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                if match:
                    command = match.group(1).strip()
                    try:
                        # Basic validation
                        if not command.startswith('msfconsole'):
                            return None, "Only msfconsole commands are allowed"
                        
                        # Check for dangerous characters (Avoiding command injection)
                        if any(char in command for char in ['&', '|', '$', '>', '<']):
                            return None, "Dangerous characters detected in command"
                        
                        return command, None
                    except Exception as e:
                        return None, f"Parsing error: {str(e)}"

            return None, "No Metasploit command found in text"

# class NovitaChatLLM(LLM):
#     """Custom LLM Wrapper for Novita InferenceClient, allowing client injection."""

#     model: str = Field(..., description="Model name for Novita API")
#     enable_stream: bool = Field(False, description="Enable response streaming")  # Renamed from `stream`
#     max_tokens: int = Field(100, description="Max tokens in response")
#     client: Union[Any] #InferenceClient  # Injected client instance

#     def __init__(self, client: InferenceClient, model: str, enable_stream: bool = False, max_tokens: int = 100, **kwargs):
#         """Initialize with an externally created InferenceClient."""
#         super().__init__(model=model, enable_stream=enable_stream, max_tokens=max_tokens, client=client, **kwargs)
#         self.client = client  # Injected client

#     def _call(self, prompt: str, stop: List[str] = None, **kwargs: Any) -> str:
#         """Send a prompt to Novita LLM and return the generated response."""
#         messages = [
#             {"role": "system", "content": "Act like you are a helpful assistant."},
#             {"role": "user", "content": prompt}
#         ]

#         response = self.client.chat.completions.create(
#             model=self.model,
#             messages=messages,
#             stream=self.enable_stream,  # Updated to use `enable_stream`
#             max_tokens=self.max_tokens
#         )
        
#         return response.choices[0].message.content if response.choices else ""

#     @property
#     def _identifying_params(self) -> Dict[str, Any]:
#         """Return model-specific identifying parameters."""
#         return {"provider": "novita", "model": self.model}

#     @property
#     def _llm_type(self) -> str:
#         """Define model type for LangChain."""
#         return "novita_chat"

class NovitaChatLLM(LLM):
    """Custom LLM wrapper supporting both HuggingFace InferenceClient and OpenAI client."""

    model: str = Field(..., description="Model name for Novita API")
    enable_stream: bool = Field(False, description="Enable response streaming")
    max_tokens: int = Field(100, description="Max tokens in response")
    client: Union[Any]  # Can be huggingface_hub.InferenceClient or openai.OpenAI

    def __init__(self, client: Any, model: str, enable_stream: bool = False, max_tokens: int = 100, **kwargs):
        super().__init__(model=model, enable_stream=enable_stream, max_tokens=max_tokens, client=client, **kwargs)
        self.client = client

    def _call(self, prompt: str, stop: List[str] = None, **kwargs: Any) -> str:
        """Send a prompt to Novita LLM and return the generated response."""
        messages = [
            {"role": "system", "content": "Act like you are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]

        try:
            # OpenAI-compatible client
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=self.enable_stream,
                max_tokens=self.max_tokens,
                **kwargs
            )
        except AttributeError:
            # HuggingFace InferenceClient fallback
            response = self.client.chat_completions.create(
                model=self.model,
                messages=messages,
                stream=self.enable_stream,
                max_tokens=self.max_tokens,
                **kwargs
            )

        # Handle streamed vs non-streamed output
        if self.enable_stream:
            return "".join([
                chunk.choices[0].delta.get("content", "")
                for chunk in response
                if chunk.choices and chunk.choices[0].delta.get("content")
            ])
        else:
            return response.choices[0].message.content if response.choices else ""

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        return {"provider": "novita", "model": self.model}

    @property
    def _llm_type(self) -> str:
        return "novita_chat"