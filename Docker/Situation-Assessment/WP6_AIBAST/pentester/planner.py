import os
import networkx as nx
import json
# from langchain.schema import HumanMessage
from langchain.prompts import PromptTemplate
from pentester.neo4j_connector import Neo4jConnector
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from .utils import LLMtoCommand, Task
from typing import List, Dict
from .prompt_templates import nmap_planner_template, msf_planner_template
from .logging_setup import get_logger

logger = get_logger(__name__)


class Planner:
    def __init__(self, planner_llm, neo4j, mode="scan"):
        """Initialize the memory agent with an execution LLM and a network graph."""
        self.planner_llm = planner_llm
        self.network_graph = nx.DiGraph()
        self.parser = LLMtoCommand()
        self.current_task_path = "pentester/planner_tasks.json" 
        self.full_task_path = "pentester/planner_tasks_full.json"
        self._mode = mode
        self.neo4j: Neo4jConnector = neo4j

        # Define the LLM-based extraction chain
        self.scan_prompt = PromptTemplate(
            input_variables=["network_data", "task_id"],
            template=nmap_planner_template,
        )
        self.scan_chain = self.scan_prompt | self.planner_llm 

                # Define the LLM-based extraction chain
        self.xploit_prompt = PromptTemplate(
            input_variables=["network_data", "task_id"],
            template=msf_planner_template,
        )
        self.xploit_chain = self.xploit_prompt | self.planner_llm  # Memory save chain

    def process_graph_data(self, network_data: dict):
        """Process the input planner infos and generate tasks."""
        # Collect output from each planner task

        task_id = 1  # default if no tasks

        if os.path.exists(self.current_task_path):
            with open(self.current_task_path, "r") as f:
                data = json.load(f)

            tasks = data.get("tasks", [])
            if tasks:
                last_task = tasks[-1]
                task_id = last_task.get("task_id", 0) + 1
        # Build the formatted string
        # Format the network data for LLM + human readability
        formatted_lines = ["# Network Task Overview", ""]  # Title

        for ip, tasks in network_data.items():
            formatted_lines.append(f"## Node: {ip}")
            formatted_lines.append("**Assigned Tasks:**")
            formatted_lines.append(f"{tasks}")
            formatted_lines.append("")  # Add space between entries

        llm_network_data = "\n".join(formatted_lines)

        # Debug: Print the formatted network data
        #print(llm_network_data)
        # Get the last task
        ### scanning or xploit mode 
        if self._mode == "scan":
            # Use the planner LLM to analyze the collected output data
            generated_tasks = self.scan_chain.invoke(
                {"network_data": llm_network_data, "last_id": task_id}
            )
        elif self._mode == "xploit":
                        # Use the planner LLM to analyze the collected output data
            generated_tasks = self.xploit_chain.invoke(
                {"network_data": llm_network_data, "last_id": task_id}
            )
        print(generated_tasks)
        self.add_generated_to_tasks(json.loads(generated_tasks))
        
        tasks_list = self.convert_to_Tasks(json.loads(generated_tasks))

        return tasks_list

    def add_generated_to_tasks(self, f_tasks: List[Dict]):
        """
        Add a list of task dictionaries to the full task list,
        and separately save only the newly generated tasks.
        """
        # Load existing tasks from full_task_path if it exists
        try:
            with open(self.full_task_path, "r") as file:
                existing_data = json.load(file)
                task_list = existing_data.get("tasks", [])
        except FileNotFoundError:
            task_list = []

        # Add new tasks to the full list
        task_list.extend(f_tasks)

        # Save the full updated list to full_task_path
        with open(self.full_task_path, "w") as file:
            json.dump({"tasks": task_list}, file, indent=4)

        # Save only the new tasks to current_task_path (overwrite)
        with open(self.current_task_path, "w") as file:
            json.dump({"tasks": f_tasks}, file, indent=4)

        #we return the dict 

    def convert_to_Tasks(self, f_tasks: List[Dict]):
        Task_list = []
        for idx, task_dict in enumerate(f_tasks):
            try:
                task = Task(**task_dict)  # Corrected the syntax from `vars**(task)` to `**task_dict`
                Task_list.append(task)
            except TypeError as e:
                print(f"[Error] Failed to convert task at index {idx}: {e}")
            except Exception as e:
                print(f"[Unexpected Error] While converting task at index {idx}: {e}")
        return Task_list

